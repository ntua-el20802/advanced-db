{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD_RSYBltEGS"
   },
   "source": [
    "### Example 1 - Wordcount (Plain Map/Reduce)\n",
    "\n",
    "**Wordcount**: find the number of occurences of each word in a body of text.\n",
    "\n",
    "#### Data format\n",
    "The file `text.txt` contains the following short text:\n",
    "\n",
    "```\n",
    "this is a text file with random words like text , words , like this is an example of a text file\n",
    "```\n",
    "\n",
    "The data is stored in HDFS: `s3://initial-notebook-data-bucket-dblab-905418150721/example_data/text.txt`.\n",
    "\n",
    "We use the **RDD API**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "id": "2uaG6GoZthpT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>324</td><td>application_1761923966900_0339</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0339/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-43.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0339_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0e878a883843e88f5b4f5156d795bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1c05294f0f49648d471c8f688269f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2), ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1), ('with', 1), ('random', 1), ('example', 1)]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wordcount example\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "wordcount = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/text.txt\") \\\n",
    "    .flatMap(lambda x: x.split(\" \")) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(wordcount.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8AUWoSxtEGZ"
   },
   "source": [
    "#### Code explanation\n",
    "- First, we create a `sparkSession` and a `SparkContex`:\n",
    "    - `sparkSession` is an entry point for every programming library in Spark and we need to create one in order to execute code.\n",
    "    - The `sparkContext` is an entry point specific for RDDs.\n",
    "\n",
    "- Then, the program reads the `text.txt` file from HDFS. With the use of a `lambda function` we split the data every time there is a whitespace between them.\n",
    "    - A lambda function is essentially an anonymous function we can use to write quick throwaway functions without defining or naming them.\n",
    "    - The lambda function the program uses as a `flatMap` argument: `Lambda x: x.split(\" \")`\n",
    "    - `flatMap` vs `map`: instead of creating multiple lists -> single list with all values.\n",
    "\n",
    "- Next, with the use of a `map` function we create a `(key,value)` pair for every word.\n",
    "    - We set `key = $word` and `value = 1`\n",
    "\n",
    "- We use the `reduceByKey` function: every tuple with the same `key` is sent to the same `reducer` so it **aggregate** them and create the result.\n",
    "    - In our case, if more than one tuples with the same `key` exist,  we **add** their `values`\n",
    "\n",
    "- Finally, we `sortBy` `value` (number of occurrence) and print the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlI0H8fKtEGb"
   },
   "source": [
    "### Example 2 - Simple Database\n",
    "#### Data format\n",
    "`Employees.csv` contains the ID of the employee, the name of the employee, the salary of the employee and the ID of the department that the employee works at.\n",
    "\n",
    "| ID          | Name        | Salary      | DepartmentID |\n",
    "| ----------- | ----------- | ----------- | ------------ |\n",
    "| 1           | George R    | 2000        | 1            |\n",
    "\n",
    "`Departments.csv` contains the ID of the department and the name of the department.\n",
    "\n",
    "| ID          | Name        |\n",
    "| ----------- | ----------- |\n",
    "| 1           | Dep A       |\n",
    "\n",
    "The data is stored in HDFS: `s3://initial-notebook-data-bucket-dblab-905418150721/example_data/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2SnsHl2tEGc"
   },
   "source": [
    "#### QUERY 1: Find the 5 worst paid employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PvMh9S_YveAp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f448107aec64144b3fc54c17f1393bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(550, ['6', 'Jerry L', '3']), (1000, ['2', 'John K', '2']), (1000, ['7', 'Marios K', '1']), (1050, ['5', 'Helen K', '2']), (1500, ['10', 'Yiannis T', '1'])]\n",
      "===========================================================================================================================\n",
      "(550, ['6', 'Jerry L', '3'])\n",
      "(1000, ['2', 'John K', '2'])\n",
      "(1000, ['7', 'Marios K', '1'])\n",
      "(1050, ['5', 'Helen K', '2'])\n",
      "(1500, ['10', 'Yiannis T', '1'])\n",
      "(2000, ['1', 'George R', '1'])\n",
      "(2100, ['3', 'Mary T', '1'])\n",
      "(2100, ['4', 'George T', '1'])\n",
      "(2500, ['8', 'George K', '2'])\n",
      "(2500, ['11', 'Antonis T', '2'])\n",
      "(3500, ['9', 'Vasilios D', '3'])"
     ]
    }
   ],
   "source": [
    "# Implementation 1: RDD API\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 1 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "    \n",
    "# Load and process data\n",
    "# CSV Columns: \"id\", \"name\", \"salary\", \"dep_id\"\n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\"))) # Split lines into a list of elements -> delimiter: \",\"\n",
    "# print(employees.collect())\n",
    "\n",
    "# Create tupples: (salary, [id, name, dep_id]); then sortByKey\n",
    "# Αντιστοίχιση στηλών:\n",
    "#   x[0] = id\n",
    "#   x[1] = name\n",
    "#   x[2] = salary\n",
    "#   x[3] = dep_id\n",
    "sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]]]) \\\n",
    "                    .sortByKey()\n",
    "\n",
    "# Different output options\n",
    "print(sorted_employees.take(5))\n",
    "print(\"===========================================================================================================================\")\n",
    "for item in sorted_employees.coalesce(1).collect():\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Omicu5ytEGh"
   },
   "source": [
    "Things to consider:\n",
    "-  `flatMmap` vs `map`: why `map` here?\n",
    "- what does the second `lambda` function do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4JAwBSsRvo4c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18bdd6a5a7e4e24a5bc478b88b309ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- dep_id: integer (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (4)\n",
      "+- Sort (3)\n",
      "   +- Exchange (2)\n",
      "      +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [4]: [id#19, name#20, salary#21, dep_id#22]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv]\n",
      "ReadSchema: struct<id:int,name:string,salary:float,dep_id:int>\n",
      "\n",
      "(2) Exchange\n",
      "Input [4]: [id#19, name#20, salary#21, dep_id#22]\n",
      "Arguments: rangepartitioning(salary#21 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=7]\n",
      "\n",
      "(3) Sort\n",
      "Input [4]: [id#19, name#20, salary#21, dep_id#22]\n",
      "Arguments: [salary#21 ASC NULLS FIRST], true, 0\n",
      "\n",
      "(4) AdaptiveSparkPlan\n",
      "Output [4]: [id#19, name#20, salary#21, dep_id#22]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+---+---------+------+------+\n",
      "| id|     name|salary|dep_id|\n",
      "+---+---------+------+------+\n",
      "|  6|  Jerry L| 550.0|     3|\n",
      "|  2|   John K|1000.0|     2|\n",
      "|  7| Marios K|1000.0|     1|\n",
      "|  5|  Helen K|1050.0|     2|\n",
      "| 10|Yiannis T|1500.0|     1|\n",
      "+---+---------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "===========================================================================================================================\n",
      "'Sort ['salary ASC NULLS FIRST], true\n",
      "+- Relation [id#19,name#20,salary#21,dep_id#22] csv\n",
      "\n",
      "Sort [salary#21 ASC NULLS FIRST], true\n",
      "+- Relation [id#19,name#20,salary#21,dep_id#22] csv\n",
      "\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [salary#21 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(salary#21 ASC NULLS FIRST, 1000), ENSURE_REQUIREMENTS, [plan_id=7]\n",
      "      +- FileScan csv [id#19,name#20,salary#21,dep_id#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/example_data/empl..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:int,name:string,salary:float,dep_id:int>"
     ]
    }
   ],
   "source": [
    "# Implementation 2: DataFrame API\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 1 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the employees DataFrame\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "])\n",
    "\n",
    "\n",
    "employees_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv\", \\\n",
    "    header=False, \\\n",
    "    schema=employees_schema)\n",
    "# Use \"printSchema()\" to display the datatypes of dataframes:\n",
    "employees_df.printSchema()\n",
    "\n",
    "# Alternative way to read csv:\n",
    "employees_df = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(employees_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv\")\n",
    "\n",
    "sorted_employess_df = employees_df.sort(col(\"salary\"))\n",
    "# Use \"explain()\" to display physical plan:\n",
    "sorted_employess_df.explain(mode=\"formatted\")\n",
    "sorted_employess_df.show(5)\n",
    "\n",
    "print(\"===========================================================================================================================\")\n",
    "# To experiment more with the optimizer: \n",
    "print(sorted_employess_df._jdf.queryExecution().logical().toString()) #Get logical plan\n",
    "print(sorted_employess_df._jdf.queryExecution().optimizedPlan().toString()) #Get optimized plan\n",
    "print(sorted_employess_df._jdf.queryExecution().executedPlan().toString()) # Get physical plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWjNHPYDxeQe"
   },
   "source": [
    "Remember to use `explain()` to check if the physical plan is what you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw8Y_0qptEGj"
   },
   "source": [
    "#### QUERY 2: Find the 3 best paid employees from \"Dep A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-meC7o8Ww0x4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78002e8e9c494b01b2d9a6eaa61f1734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2100, ['3', 'Mary T']), (2100, ['4', 'George T']), (2000, ['1', 'George R'])]"
     ]
    }
   ],
   "source": [
    "# Implementation 1: RDD API\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 2 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "\n",
    "    \n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\"))) # → [emp_id, emp_name, salary, dep_id]\n",
    "departments = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/departments.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\"))) # → [id, dpt_name]\n",
    "\n",
    "# =======================\n",
    "# SCHEMA DETAILS:\n",
    "# employees:   \"emp_id\", \"emp_name\", \"salary\", \"dep_id\"\n",
    "# departments: \"id\", \"dpt_name\"\n",
    "#\n",
    "# Contents of employees RDD per row:\n",
    "#   x[0] = emp_id\n",
    "#   x[1] = emp_name\n",
    "#   x[2] = salary\n",
    "#   x[3] = dep_id\n",
    "#\n",
    "# Contents of departments RDD per row:\n",
    "#   x[0] = id\n",
    "#   x[1] = dpt_name\n",
    "# =======================\n",
    "\n",
    "# Filter & only keep departments named \"Dep A\"\n",
    "depA = departments.filter(lambda x: x[1] == \"Dep A\")\n",
    "\n",
    "# (k, v) -> (dep_id, [emp_id, emp_name, salary])\n",
    "employees_formatted = employees.map(lambda x: [x[3] , [x[0],x[1],x[2]] ] )\n",
    "# (k, v) -> (id, [dpt_name])\n",
    "depA_formatted = depA.map(lambda x: [x[0], [x[1]]])\n",
    "# print(employees_formatted.collect())\n",
    "# print(depA_formatted.collect())\n",
    "\n",
    "\n",
    "# Guess the data format????\n",
    "joined_data = employees_formatted.join(depA_formatted)\n",
    "# print(joined_data.collect())\n",
    "\n",
    "get_employees = joined_data.map(lambda x: (x[1][0]))\n",
    "# print(get_employees.collect())\n",
    "\n",
    "sorted_employees= get_employees.map(lambda x: [int(x[2]),[x[0], x[1]] ] ) \\\n",
    "                                .sortByKey(ascending=False)\n",
    "print(sorted_employees.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WW5_0kkttEGn"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243449ad24de48c49d1a656bbcd7c935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    name|salary|\n",
      "+--------+------+\n",
      "|  Mary T|2100.0|\n",
      "|George T|2100.0|\n",
      "|George R|2000.0|\n",
      "+--------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Sort (10)\n",
      "   +- Exchange (9)\n",
      "      +- Project (8)\n",
      "         +- BroadcastHashJoin Inner BuildRight (7)\n",
      "            :- Filter (2)\n",
      "            :  +- Scan csv  (1)\n",
      "            +- BroadcastExchange (6)\n",
      "               +- Project (5)\n",
      "                  +- Filter (4)\n",
      "                     +- Scan csv  (3)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [name#50, salary#51, dep_id#52]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv]\n",
      "PushedFilters: [IsNotNull(dep_id)]\n",
      "ReadSchema: struct<name:string,salary:float,dep_id:int>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [name#50, salary#51, dep_id#52]\n",
      "Condition : isnotnull(dep_id#52)\n",
      "\n",
      "(3) Scan csv \n",
      "Output [2]: [id#57, name#58]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/example_data/departments.csv]\n",
      "PushedFilters: [IsNotNull(name), EqualTo(name,Dep A), IsNotNull(id)]\n",
      "ReadSchema: struct<id:int,name:string>\n",
      "\n",
      "(4) Filter\n",
      "Input [2]: [id#57, name#58]\n",
      "Condition : ((isnotnull(name#58) AND (name#58 = Dep A)) AND isnotnull(id#57))\n",
      "\n",
      "(5) Project\n",
      "Output [1]: [id#57]\n",
      "Input [2]: [id#57, name#58]\n",
      "\n",
      "(6) BroadcastExchange\n",
      "Input [1]: [id#57]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=142]\n",
      "\n",
      "(7) BroadcastHashJoin\n",
      "Left keys [1]: [dep_id#52]\n",
      "Right keys [1]: [id#57]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(8) Project\n",
      "Output [2]: [name#50, salary#51]\n",
      "Input [4]: [name#50, salary#51, dep_id#52, id#57]\n",
      "\n",
      "(9) Exchange\n",
      "Input [2]: [name#50, salary#51]\n",
      "Arguments: rangepartitioning(salary#51 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=146]\n",
      "\n",
      "(10) Sort\n",
      "Input [2]: [name#50, salary#51]\n",
      "Arguments: [salary#51 DESC NULLS LAST], true, 0\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [2]: [name#50, salary#51]\n",
      "Arguments: isFinalPlan=false"
     ]
    }
   ],
   "source": [
    "# Implementation 2: SQL API\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "])\n",
    "\n",
    "employees_df = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(employees_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv\")\n",
    "\n",
    "departments_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "])\n",
    "\n",
    "departments_df = spark.read.format('csv') \\\n",
    "    .options(header='false') \\\n",
    "    .schema(departments_schema) \\\n",
    "    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/departments.csv\")\n",
    "\n",
    "# To utilize as SQL tables\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "### USE TEMPORARY TABLE depA ###\n",
    "id_query = \"SELECT departments.id, departments.name \\\n",
    "    FROM departments \\\n",
    "    WHERE departments.name == 'Dep A'\"\n",
    "\n",
    "depA_id = spark.sql(id_query)\n",
    "depA_id.createOrReplaceTempView(\"depA\")\n",
    "inner_join_query = \"SELECT employees.name, employees.salary \\\n",
    "    FROM employees \\\n",
    "    INNER JOIN depA ON employees.dep_id == depA.id \\\n",
    "    ORDER BY employees.salary DESC\"\n",
    "joined_data = spark.sql(inner_join_query)\n",
    "################################\n",
    "### OR NOT ###\n",
    "# inner_join_query = \"\"\"\n",
    "#     SELECT employees.name, employees.salary\n",
    "#     FROM employees\n",
    "#     INNER JOIN departments ON employees.dep_id == departments.id\n",
    "#     WHERE departments.name == 'Dep A'\n",
    "#     ORDER BY employees.salary DESC\n",
    "# \"\"\"\n",
    "# joined_data = spark.sql(inner_join_query)\n",
    "################################\n",
    "joined_data.show(3)\n",
    "joined_data.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEt9QFEtEGo"
   },
   "source": [
    "### Example 3 - Simple Database with a twist (DataFrame UDFs)\n",
    "\n",
    "Sometimes we need to define functions that process the values of specific columns of a single row.\n",
    "\n",
    "Motivating example: a database with salaries and bonuses for our employees:\n",
    "\n",
    "| ID          | Name        | Salary      | DepartmentID | Bonus        |\n",
    "| ----------- | ----------- | ----------- | ------------ | ------------ |\n",
    "| 1           | George R    | 2000        | 1            | 500          |\n",
    "\n",
    "We wan to calculate the total yearly income for each one of them: `14 x Salary + Bonus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4fCm45pjzBug"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e0ce3720394002bb54df91fa45a77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['George R', 28500], ['John K', 14150], ['Mary T', 29850], ['George T', 29720], ['Helen K', 14900], ['Jerry L', 7900], ['Marios K', 14550], ['George K', 36500], ['Vasilios D', 50000], ['Yiannis T', 21450], ['Antonis T', 35620]]"
     ]
    }
   ],
   "source": [
    "# Implementation 1: RDD API\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query 3 execution\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext\n",
    "    \n",
    "employees = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees2.csv\") \\\n",
    "                .map(lambda x: (x.split(\",\")))\n",
    "# print(employees.collect())\n",
    "\n",
    "employees_yearly = employees.map(lambda x: [x[1], 14*(int(x[2]))+int(x[4])])\n",
    "\n",
    "                    \n",
    "print(employees_yearly.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L4BziM-Oz1V3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00270994b5af472aa5008d9e60864e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      name| yearly|\n",
      "+----------+-------+\n",
      "|  George R|28500.0|\n",
      "|    John K|14150.0|\n",
      "|    Mary T|29850.0|\n",
      "|  George T|29720.0|\n",
      "|   Helen K|14900.0|\n",
      "|   Jerry L| 7900.0|\n",
      "|  Marios K|14550.0|\n",
      "|  George K|36500.0|\n",
      "|Vasilios D|50000.0|\n",
      "| Yiannis T|21450.0|\n",
      "| Antonis T|35620.0|\n",
      "+----------+-------+"
     ]
    }
   ],
   "source": [
    "# Implementation 2: DataFrame API\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame query 3 execution (UDF example)\") \\\n",
    "    .getOrCreate()\n",
    "()\n",
    "employees2_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"salary\", FloatType()),\n",
    "    StructField(\"dep_id\", IntegerType()),\n",
    "    StructField(\"bonus\", FloatType())\n",
    "])\n",
    "\n",
    "\n",
    "employees_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees2.csv\", header=False, schema=employees2_schema)\n",
    "\n",
    "\n",
    "\n",
    "###  WITH UDF  ###\n",
    "def calculate_yearly_income(salary, bonus):\n",
    "    return 14*salary+bonus\n",
    "# Register the UDF\n",
    "calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())\n",
    "employees_yearly_df = employees_df \\\n",
    "    .withColumn(\"yearly\", calculate_yearly_income_udf(col(\"salary\"), col(\"bonus\"))).select(\"name\", \"yearly\")\n",
    "##################\n",
    "\n",
    "### WITHOUT UDF ###\n",
    "# employees_yearly_df = employees_df \\\n",
    "#     .withColumn(\"yearly\", (14*col(\"salary\")+col(\"bonus\"))).select(\"name\", \"yearly\")\n",
    "####################\n",
    "\n",
    "employees_yearly_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
