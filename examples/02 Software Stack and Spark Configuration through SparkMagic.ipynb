{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995df16e-2881-43c7-9394-e41c92cd99ec",
   "metadata": {},
   "source": [
    "# Understanding the Infrastructure: How It All Connects\n",
    "\n",
    "When you write code in this Jupyter notebook, you're not running Spark locally on your machine. Instead, you're interacting with a sophisticated distributed system hosted on AWS. Understanding this architecture helps you write better code and troubleshoot issues more effectively.\n",
    "\n",
    "​\n",
    "## The Technology Stack\n",
    "\n",
    "- [Jupyter Notebook (Your Interface)](https://jupyter.org/): This is the web-based interface where you write and execute code. It's hosted on AWS SageMaker, which provides managed notebook instances with pre-configured environments. You interact with it through your browser, but the actual computation happens elsewhere.\n",
    "\n",
    "- [SparkMagic (The Bridge)](https://github.com/jupyter-incubator/sparkmagic): SparkMagic is a library that enables Jupyter notebooks to communicate with remote Spark clusters. It provides the special \"magic\" commands you use (like %%configure and %%sql) and translates your notebook commands into requests that can be sent to a remote Spark cluster. Think of it as a translator between your notebook and the Spark cluster.\n",
    "\n",
    "- [Apache Livy (The REST Server)](https://github.com/apache/incubator-livy): Livy is a REST server that sits between SparkMagic and the actual Spark cluster. When you execute code in your notebook, SparkMagic sends HTTP requests to Livy, which then submits jobs to the Spark cluster and returns results back to your notebook. This REST-based approach allows multiple users to interact with the same Spark cluster safely and independently.\n",
    "\n",
    "- [Apache Spark Cluster (The Compute Engine)](https://spark.apache.org/): This is where your actual distributed computations happen. The cluster consists of a driver node (which coordinates work) and multiple executor nodes (which process data in parallel). When you configure executor instances, memory, and cores, you're specifying how this cluster should allocate resources for your job.\n",
    "\n",
    "- Amazon S3 (The Storage Layer): S3 provides scalable object storage for your data files. Notice in the notebook examples how data paths reference S3 URLs (e.g., s3://bucket-name/path/to/file). Spark reads data directly from S3, processes it across the cluster, and can write results back to S3. This separation of storage and compute is a key advantage of cloud architectures.\n",
    "\n",
    "\n",
    "## The Data Flow\n",
    "\n",
    "Here's what happens when you execute a Spark command:\n",
    "\n",
    "1. You write code in a Jupyter notebook cell and press Shift+Enter\n",
    "2. SparkMagic intercepts your code and packages it into an HTTP request\n",
    "3. The request travels over the network to the Livy REST server\n",
    "4. Livy submits your code as a job to the Spark cluster\n",
    "5. Spark driver creates a logical execution plan and distributes tasks to executors\n",
    "6. Executors read data from S3, process it in parallel, and potentially write results back\n",
    "7. Results flow back through Livy to SparkMagic to your notebook for display\n",
    "\n",
    "## Why This Architecture?\n",
    "\n",
    "- Resource Efficiency: Instead of running a full Spark cluster on your laptop, you share powerful cloud resources with other users. The cluster can scale up or down based on workload demands.\n",
    "- Separation of Concerns: Your notebook environment is separate from compute resources. This means you can have a lightweight notebook interface while executing jobs on clusters with hundreds of gigabytes of RAM and dozens of CPU cores.\n",
    "- Data Locality: By hosting everything in AWS, data doesn't need to move between your local machine and the cloud. S3 storage stays close to Spark compute, minimizing data transfer time.\n",
    "- Multi-Language Support: The same Spark cluster can execute jobs from notebooks written in Python, Scala, or R, all managed through the same Livy interface.\n",
    "\n",
    "## Practical Implications for Your Code\n",
    "\n",
    "- Latency: Because code execution involves network communication between components, there's a small delay between submitting code and seeing results. This is normal and usually negligible for data processing workloads.\n",
    "- State Management: Your SparkContext (sc) and SQL context (sqlContext) live on the remote cluster, not in your notebook. If you restart the Livy session (with %%configure -f), you lose all cached data and temporary views.\n",
    "- Data Access: Always use S3 paths for file operations in this environment. The Spark cluster can efficiently read from S3, but it cannot access files on your local machine unless you explicitly upload them.\n",
    "- Debugging: When errors occur, they may be from different layers—Jupyter, SparkMagic, Livy, or Spark itself. The error messages will help you identify which component encountered the issue.\n",
    "\n",
    "## AWS SageMaker's Role\n",
    "\n",
    "SageMaker provides the managed notebook infrastructure, handling the provisioning, configuration, and maintenance of your Jupyter environment. It automatically sets up SparkMagic, connects it to Livy endpoints, and manages authentication to AWS services like S3. This allows you to focus on writing Spark code rather than configuring infrastructure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b1d67-7dcc-4479-a1b4-d279b100ad74",
   "metadata": {},
   "source": [
    "# Configuring Your Spark Session\n",
    "\n",
    "Before executing Spark workloads, you can customize how Spark allocates cluster resources using the %%configure magic command. This allows you to control critical parameters like memory allocation, CPU cores, and the number of executors.\n",
    "\n",
    "\n",
    "## Basic Configuration Syntax\n",
    "\n",
    "The `%%configure` magic must be placed in its own cell at the beginning of your notebook, before running any Spark code. Here's the basic structure:\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0a0dd2-41c4-4c04-9ec1-06750d187f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>5</td><td>application_1761923966900_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-24.eu-central-1.compute.internal:20888/proxy/application_1761923966900_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-75.eu-central-1.compute.internal:8042/node/containerlogs/container_1761923966900_0006_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb43c78-3c09-4f91-babf-662679f5425c",
   "metadata": {},
   "source": [
    "## Understanding the -f Flag\n",
    "\n",
    "The -f (force) flag tells Spark to restart the session with new configurations. This is necessary if you've already executed Spark commands and want to change settings. Important: Using -f will lose all progress from previous Spark jobs in that session, so plan your configuration carefully.\n",
    "\n",
    "​\n",
    "## Key Configuration Parameters\n",
    "\n",
    "- `spark.executor.instances`: Specifies the number of executor processes to launch across the cluster. In the example above, we request 4 executors. More executors enable greater parallelism but consume more cluster resources.\n",
    "- `spark.executor.memory`: Sets the amount of memory allocated to each executor. The value \"2g\" means 2 gigabytes per executor. Consider your data size when setting this—larger datasets require more memory to avoid out-of-memory errors.\n",
    "- `spark.executor.cores`: Determines how many CPU cores each executor can use. Setting this to \"1\" means each executor runs tasks sequentially. Higher values (e.g., 2-4) allow concurrent task execution within each executor, improving performance for CPU-intensive operations.\n",
    "\n",
    "## Additional Common Parameters\n",
    "\n",
    "You can also configure:\n",
    "- `driverMemory`: Memory for the driver process (e.g., \"4g\")\n",
    "- `driverCores`: CPU cores for the driver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
