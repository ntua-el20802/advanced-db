{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a017b2da-9b6e-4a86-a684-8625f6f07721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive'}, 'driverMemory': '2G', 'executorMemory': '8G', 'executorCores': 4, 'numExecutors': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1437</td><td>application_1765289937462_1425</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1425/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1425_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1469</td><td>application_1765289937462_1456</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1456/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1456_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1473</td><td>application_1765289937462_1459</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1459/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1459_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1474</td><td>application_1765289937462_1460</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1460/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1460_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1475</td><td>application_1765289937462_1461</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1461/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1461_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"2G\",\n",
    "    \"executorMemory\": \"8G\",\n",
    "    \"executorCores\": 4,\n",
    "    \"numExecutors\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3866251-d588-41be-a5c2-b387ed892a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1476</td><td>application_1765289937462_1462</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1462/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1462_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d694879ffa142db850cd40d1e6eff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca0dc82fd474a48845affae22f82b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"query4_1\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8cd7a3-0998-4a2b-895c-94958fa547c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a361f01fc1442b6b9d64dc4f18631a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_old_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "crimes_new_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "crimes_df = crimes_old_df.unionByName(crimes_new_df)\n",
    "\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bff095a-a2a3-4209-95fd-c84eb77b8f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebaaa75916a454ca3ea22f72998e2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up 1: 5.0884 sec\n",
      "Warm-up 2: 0.0464 sec\n",
      "Warm-up 3: 0.0451 sec\n",
      "Run 1: 0.0445 sec\n",
      "Run 2: 0.0450 sec\n",
      "Run 3: 0.0454 sec\n",
      "Run 4: 0.0453 sec\n",
      "Run 5: 0.0414 sec\n",
      "Mean time (excluding warm-ups): 0.0443 sec\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- BroadcastHashJoin Inner BuildRight (12)\n",
      "   :- Union (7)\n",
      "   :  :- Project (3)\n",
      "   :  :  +- Filter (2)\n",
      "   :  :     +- Scan csv  (1)\n",
      "   :  +- Project (6)\n",
      "   :     +- Filter (5)\n",
      "   :        +- Scan csv  (4)\n",
      "   +- BroadcastExchange (11)\n",
      "      +- Project (10)\n",
      "         +- Filter (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [AREA#33, LAT#55, LON#56]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0))), IsNotNull(AREA)]\n",
      "ReadSchema: struct<AREA:int,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [AREA#33, LAT#55, LON#56]\n",
      "Condition : (((isnotnull(LAT#55) AND isnotnull(LON#56)) AND (NOT (LAT#55 = 0.0) OR NOT (LON#56 = 0.0))) AND isnotnull(AREA#33))\n",
      "\n",
      "(3) Project\n",
      "Output [4]: [AREA#33, LAT#55, LON#56,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#2294]\n",
      "Input [3]: [AREA#33, LAT#55, LON#56]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [AREA#107, LAT#129, LON#130]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0))), IsNotNull(AREA)]\n",
      "ReadSchema: struct<AREA:int,LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [AREA#107, LAT#129, LON#130]\n",
      "Condition : (((isnotnull(LAT#129) AND isnotnull(LON#130)) AND (NOT (LAT#129 = 0.0) OR NOT (LON#130 = 0.0))) AND isnotnull(AREA#107))\n",
      "\n",
      "(6) Project\n",
      "Output [4]: [AREA#107, LAT#129, LON#130,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#2448]\n",
      "Input [3]: [AREA#107, LAT#129, LON#130]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [6]: [X#206, Y#207, FID#208, DIVISION#209, LOCATION#210, PREC#211]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "PushedFilters: [IsNotNull(PREC)]\n",
      "ReadSchema: struct<X:double,Y:double,FID:int,DIVISION:string,LOCATION:string,PREC:int>\n",
      "\n",
      "(9) Filter\n",
      "Input [6]: [X#206, Y#207, FID#208, DIVISION#209, LOCATION#210, PREC#211]\n",
      "Condition : isnotnull(PREC#211)\n",
      "\n",
      "(10) Project\n",
      "Output [7]: [X#206 AS lon#2269, Y#207 AS lat#2276, FID#208, DIVISION#209, LOCATION#210, PREC#211,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#2286]\n",
      "Input [6]: [X#206, Y#207, FID#208, DIVISION#209, LOCATION#210, PREC#211]\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [7]: [lon#2269, lat#2276, FID#208, DIVISION#209, LOCATION#210, PREC#211, station_geom#2286]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[5, int, false] as bigint)),false), [plan_id=4077]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [AREA#33]\n",
      "Right keys [1]: [PREC#211]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [11]: [AREA#33, LAT#55, LON#56, crime_geom#2294, lon#2269, lat#2276, FID#208, DIVISION#209, LOCATION#210, PREC#211, station_geom#2286]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+---------------+----------------+------+\n",
      "|DIVISION       |average_distance|count |\n",
      "+---------------+----------------+------+\n",
      "|77TH STREET    |2697.177        |207381|\n",
      "|SOUTHWEST      |2700.115        |193143|\n",
      "|PACIFIC        |3879.004        |172085|\n",
      "|CENTRAL        |1122.233        |167941|\n",
      "|NORTH HOLLYWOOD|2613.796        |165264|\n",
      "+---------------+----------------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import time, gc\n",
    "\n",
    "# Create/Init Sedona\n",
    "spark = SedonaContext.create(spark)\n",
    "\n",
    "# ----------------------------\n",
    "# Query 4 (DataFrame API)\n",
    "# ----------------------------\n",
    "\n",
    "# Rename X,Y -> lon,lat\n",
    "stations_geo = (\n",
    "    stations_df\n",
    "    .withColumnRenamed(\"X\", \"lon\")\n",
    "    .withColumnRenamed(\"Y\", \"lat\")\n",
    ")\n",
    "\n",
    "# Filter crimes: remove nulls + \"Null Island\"\n",
    "crimes_df_clean = crimes_df.filter(\n",
    "    F.col(\"LAT\").isNotNull() &\n",
    "    F.col(\"LON\").isNotNull() &\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Keep only needed columns\n",
    "crimes_geo = crimes_df_clean.select(\"AREA\", \"LAT\", \"LON\")\n",
    "\n",
    "# Create geometries\n",
    "stations_geo = stations_geo.withColumn(\"station_geom\", ST_Point(col(\"lon\"), col(\"lat\")))\n",
    "crimes_geo   = crimes_geo.withColumn(\"crime_geom\",   ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "# Join on AREA = PREC\n",
    "joined_df = crimes_geo.alias(\"c\").join(\n",
    "    stations_geo.alias(\"s\"),\n",
    "    col(\"c.AREA\") == col(\"s.PREC\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "final_df = joined_df.select(\n",
    "    col(\"s.FID\").alias(\"FID\"),\n",
    "    col(\"s.DIVISION\").alias(\"DIVISION\"),\n",
    "    col(\"s.LOCATION\").alias(\"LOCATION\"),\n",
    "    col(\"s.PREC\").alias(\"PREC\"),\n",
    "    col(\"s.station_geom\").alias(\"station_geom\"),\n",
    "    col(\"c.crime_geom\").alias(\"crime_geom\")\n",
    ")\n",
    "\n",
    "with_dist = final_df.withColumn(\n",
    "    \"distance_m\",\n",
    "    ST_DistanceSphere(col(\"station_geom\"), col(\"crime_geom\"))\n",
    ")\n",
    "\n",
    "result = (\n",
    "    with_dist\n",
    "    .groupBy(\"DIVISION\")\n",
    "    .agg(\n",
    "        F.round(F.avg(\"distance_m\"), 3).alias(\"average_distance\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Timing: 3 warm-ups + 5 runs\n",
    "# ----------------------------\n",
    "warmups = 3\n",
    "runs = 5\n",
    "times = []\n",
    "\n",
    "for i in range(warmups + runs):\n",
    "    start = time.time()\n",
    "    rows = result.collect()   # action\n",
    "    dt = time.time() - start\n",
    "\n",
    "    del rows\n",
    "    gc.collect()\n",
    "\n",
    "    if i < warmups:\n",
    "        print(f\"Warm-up {i+1}: {dt:.4f} sec\")\n",
    "    else:\n",
    "        times.append(dt)\n",
    "        print(f\"Run {i-warmups+1}: {dt:.4f} sec\")\n",
    "\n",
    "mean_time = sum(times) / len(times)\n",
    "print(f\"Mean time (excluding warm-ups): {mean_time:.4f} sec\")\n",
    "\n",
    "# ----------------------------\n",
    "# Explain only at the end\n",
    "# ----------------------------\n",
    "\n",
    "joined_df.explain(\"formatted\")\n",
    "\n",
    "# Optional: show sample output\n",
    "result.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead648f-41d8-489d-a4c5-fb78b3d36b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3ff62-2de0-4a10-bbeb-adaff88e9c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
