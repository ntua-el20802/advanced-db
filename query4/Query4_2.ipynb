{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a017b2da-9b6e-4a86-a684-8625f6f07721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive'}, 'driverMemory': '2G', 'executorMemory': '4G', 'executorCores': 2, 'numExecutors': 2, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1437</td><td>application_1765289937462_1425</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1425/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1425_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1469</td><td>application_1765289937462_1456</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1456/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1456_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1473</td><td>application_1765289937462_1459</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1459/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1459_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1474</td><td>None</td><td>pyspark</td><td>starting</td><td></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"2G\",\n",
    "    \"executorMemory\": \"4G\",\n",
    "    \"executorCores\": 2,\n",
    "    \"numExecutors\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3866251-d588-41be-a5c2-b387ed892a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1475</td><td>application_1765289937462_1461</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1461/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1461_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cec998c3f244dbbd6eab2fb9027a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714990952e7c40488321a873e7b079c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"query4_1\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8cd7a3-0998-4a2b-895c-94958fa547c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695a770f1ec5480cb10803bb8b5e90d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_old_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "crimes_new_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "crimes_df = crimes_old_df.unionByName(crimes_new_df)\n",
    "\n",
    "stations_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a109fc-9e35-4c75-bc90-04f3e6ef3870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa87e500d03d4f48bd14685bba704440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up 1: 11.2836 sec\n",
      "Warm-up 2: 0.0774 sec\n",
      "Warm-up 3: 0.0768 sec\n",
      "Run 1: 0.0750 sec\n",
      "Run 2: 0.0711 sec\n",
      "Run 3: 0.0648 sec\n",
      "Run 4: 0.0743 sec\n",
      "Run 5: 0.0792 sec\n",
      "Mean time (excluding warm-ups): 0.0729 sec\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (13)\n",
      "+- BroadcastHashJoin Inner BuildRight (12)\n",
      "   :- Union (7)\n",
      "   :  :- Project (3)\n",
      "   :  :  +- Filter (2)\n",
      "   :  :     +- Scan csv  (1)\n",
      "   :  +- Project (6)\n",
      "   :     +- Filter (5)\n",
      "   :        +- Scan csv  (4)\n",
      "   +- BroadcastExchange (11)\n",
      "      +- Project (10)\n",
      "         +- Filter (9)\n",
      "            +- Scan csv  (8)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [3]: [AREA#447, LAT#469, LON#470]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0))), IsNotNull(AREA)]\n",
      "ReadSchema: struct<AREA:int,LAT:double,LON:double>\n",
      "\n",
      "(2) Filter\n",
      "Input [3]: [AREA#447, LAT#469, LON#470]\n",
      "Condition : (((isnotnull(LAT#469) AND isnotnull(LON#470)) AND (NOT (LAT#469 = 0.0) OR NOT (LON#470 = 0.0))) AND isnotnull(AREA#447))\n",
      "\n",
      "(3) Project\n",
      "Output [4]: [AREA#447, LAT#469, LON#470,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#1770]\n",
      "Input [3]: [AREA#447, LAT#469, LON#470]\n",
      "\n",
      "(4) Scan csv \n",
      "Output [3]: [AREA#521, LAT#543, LON#544]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv]\n",
      "PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Or(Not(EqualTo(LAT,0.0)),Not(EqualTo(LON,0.0))), IsNotNull(AREA)]\n",
      "ReadSchema: struct<AREA:int,LAT:double,LON:double>\n",
      "\n",
      "(5) Filter\n",
      "Input [3]: [AREA#521, LAT#543, LON#544]\n",
      "Condition : (((isnotnull(LAT#543) AND isnotnull(LON#544)) AND (NOT (LAT#543 = 0.0) OR NOT (LON#544 = 0.0))) AND isnotnull(AREA#521))\n",
      "\n",
      "(6) Project\n",
      "Output [4]: [AREA#521, LAT#543, LON#544,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#1924]\n",
      "Input [3]: [AREA#521, LAT#543, LON#544]\n",
      "\n",
      "(7) Union\n",
      "\n",
      "(8) Scan csv \n",
      "Output [6]: [X#620, Y#621, FID#622, DIVISION#623, LOCATION#624, PREC#625]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv]\n",
      "PushedFilters: [IsNotNull(PREC)]\n",
      "ReadSchema: struct<X:double,Y:double,FID:int,DIVISION:string,LOCATION:string,PREC:int>\n",
      "\n",
      "(9) Filter\n",
      "Input [6]: [X#620, Y#621, FID#622, DIVISION#623, LOCATION#624, PREC#625]\n",
      "Condition : isnotnull(PREC#625)\n",
      "\n",
      "(10) Project\n",
      "Output [7]: [X#620 AS lon#1745, Y#621 AS lat#1752, FID#622, DIVISION#623, LOCATION#624, PREC#625,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS station_geom#1762]\n",
      "Input [6]: [X#620, Y#621, FID#622, DIVISION#623, LOCATION#624, PREC#625]\n",
      "\n",
      "(11) BroadcastExchange\n",
      "Input [7]: [lon#1745, lat#1752, FID#622, DIVISION#623, LOCATION#624, PREC#625, station_geom#1762]\n",
      "Arguments: HashedRelationBroadcastMode(List(cast(input[5, int, false] as bigint)),false), [plan_id=2528]\n",
      "\n",
      "(12) BroadcastHashJoin\n",
      "Left keys [1]: [AREA#447]\n",
      "Right keys [1]: [PREC#625]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(13) AdaptiveSparkPlan\n",
      "Output [11]: [AREA#447, LAT#469, LON#470, crime_geom#1770, lon#1745, lat#1752, FID#622, DIVISION#623, LOCATION#624, PREC#625, station_geom#1762]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n",
      "+---------------+----------------+------+\n",
      "|DIVISION       |average_distance|count |\n",
      "+---------------+----------------+------+\n",
      "|77TH STREET    |2697.177        |207381|\n",
      "|SOUTHWEST      |2700.115        |193143|\n",
      "|PACIFIC        |3879.004        |172085|\n",
      "|CENTRAL        |1122.233        |167941|\n",
      "|NORTH HOLLYWOOD|2613.796        |165264|\n",
      "+---------------+----------------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import time, gc\n",
    "\n",
    "# Create/Init Sedona\n",
    "spark = SedonaContext.create(spark)\n",
    "\n",
    "# ----------------------------\n",
    "# Query 4 (DataFrame API)\n",
    "# ----------------------------\n",
    "\n",
    "# Rename X,Y -> lon,lat\n",
    "stations_geo = (\n",
    "    stations_df\n",
    "    .withColumnRenamed(\"X\", \"lon\")\n",
    "    .withColumnRenamed(\"Y\", \"lat\")\n",
    ")\n",
    "\n",
    "# Filter crimes: remove nulls + \"Null Island\"\n",
    "crimes_df_clean = crimes_df.filter(\n",
    "    F.col(\"LAT\").isNotNull() &\n",
    "    F.col(\"LON\").isNotNull() &\n",
    "    ~((F.col(\"LAT\") == 0) & (F.col(\"LON\") == 0))\n",
    ")\n",
    "\n",
    "# Keep only needed columns\n",
    "crimes_geo = crimes_df_clean.select(\"AREA\", \"LAT\", \"LON\")\n",
    "\n",
    "# Create geometries\n",
    "stations_geo = stations_geo.withColumn(\"station_geom\", ST_Point(col(\"lon\"), col(\"lat\")))\n",
    "crimes_geo   = crimes_geo.withColumn(\"crime_geom\",   ST_Point(col(\"LON\"), col(\"LAT\")))\n",
    "\n",
    "# Join on AREA = PREC\n",
    "joined_df = crimes_geo.alias(\"c\").join(\n",
    "    stations_geo.alias(\"s\"),\n",
    "    col(\"c.AREA\") == col(\"s.PREC\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "final_df = joined_df.select(\n",
    "    col(\"s.FID\").alias(\"FID\"),\n",
    "    col(\"s.DIVISION\").alias(\"DIVISION\"),\n",
    "    col(\"s.LOCATION\").alias(\"LOCATION\"),\n",
    "    col(\"s.PREC\").alias(\"PREC\"),\n",
    "    col(\"s.station_geom\").alias(\"station_geom\"),\n",
    "    col(\"c.crime_geom\").alias(\"crime_geom\")\n",
    ")\n",
    "\n",
    "with_dist = final_df.withColumn(\n",
    "    \"distance_m\",\n",
    "    ST_DistanceSphere(col(\"station_geom\"), col(\"crime_geom\"))\n",
    ")\n",
    "\n",
    "result = (\n",
    "    with_dist\n",
    "    .groupBy(\"DIVISION\")\n",
    "    .agg(\n",
    "        F.round(F.avg(\"distance_m\"), 3).alias(\"average_distance\"),\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Timing: 3 warm-ups + 5 runs\n",
    "# ----------------------------\n",
    "warmups = 3\n",
    "runs = 5\n",
    "times = []\n",
    "\n",
    "for i in range(warmups + runs):\n",
    "    start = time.time()\n",
    "    rows = result.collect()   # action\n",
    "    dt = time.time() - start\n",
    "\n",
    "    del rows\n",
    "    gc.collect()\n",
    "\n",
    "    if i < warmups:\n",
    "        print(f\"Warm-up {i+1}: {dt:.4f} sec\")\n",
    "    else:\n",
    "        times.append(dt)\n",
    "        print(f\"Run {i-warmups+1}: {dt:.4f} sec\")\n",
    "\n",
    "mean_time = sum(times) / len(times)\n",
    "print(f\"Mean time (excluding warm-ups): {mean_time:.4f} sec\")\n",
    "\n",
    "# ----------------------------\n",
    "# Explain only at the end\n",
    "# ----------------------------\n",
    "\n",
    "joined_df.explain(\"formatted\")\n",
    "\n",
    "# Optional: show sample output\n",
    "result.show(5, truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef2f9d-c505-4b4d-ac26-ff33fe605fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
