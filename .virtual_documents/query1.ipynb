get_ipython().run_cell_magic("configure", " -f", """{
    "driverMemory": "2G",
    "executorMemory": "2G",
    "executorCores": 1,
    "numExecutors": 4
}""")


from pyspark.sql.functions import col, lower, when, desc, udf
import time
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

print(f"Executors: {sc.getConf().get('spark.executor.instances')}")
print(f"Master: {sc.master}")





crimes_old_path = "s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv"
crimes_new_path = "s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv"

crimes_old_df = spark.read.csv(crimes_old_path, header=True, inferSchema=True)
crimes_new_df = spark.read.csv(crimes_new_path, header=True, inferSchema=True)

times = []

print("Files loaded.")


# Dataframes without UDFs
spark.catalog.clearCache()

crimes_df = crimes_old_df.unionByName(crimes_new_df)
assaults_df = crimes_df.filter(lower(col("Crm Cd Desc"))
                               .contains("aggravated assault"))

assaults_grouped_df = assaults_df.withColumn(
    "Age_Group",
    when(col("Vict Age") < 18, "Children")
    .when((col("Vict Age") >= 18) & (col("Vict Age") <= 24), "Young Adults")
    .when((col("Vict Age") >= 25) & (col("Vict Age") <= 64), "Adults")
    .when(col("Vict Age") > 64, "Elderly")
)

result = assaults_grouped_df \
    .filter(col("Age_Group").isNotNull()) \
    .groupBy("Age_Group") \
    .count() \
    .orderBy(desc("count"))

start_time = time.time()
result_count = result.collect()
end_time = time.time()
times.append(end_time - start_time)

print(f"Native DF Time: {end_time - start_time:.4f} sec")


# Dataframes with UDF
spark.catalog.clearCache()

def age_group(age):
    if age is None:
        return None
    if age < 18:
        return "Children"
    elif 18 <= age <= 24:
        return "Young Adults"
    elif 25 <= age <= 64:
        return "Adults"
    else:
        return "Elderly"

age_group_udf = udf(age_group, StringType())

crimes_df_udf = crimes_old_df.unionByName(crimes_new_df)
assaults_df_udf = crimes_df_udf.filter(lower(col("Crm Cd Desc"))
                                        .contains("aggravated assault"))

udf_df = assaults_df_udf.withColumn("Age_Group", age_group_udf(col("Vict Age")))

result_udf = udf_df \
    .filter(col("Age_Group").isNotNull()) \
    .groupBy("Age_Group") \
    .count() \
    .orderBy(desc("count"))

start_time = time.time()
result_udf_count = result_udf.collect()
end_time = time.time()
times.append(end_time - start_time)

print(f"DF with UDF Time: {end_time - start_time:.4f} sec")


# RDDs
spark.catalog.clearCache()

def map_to_age_groups(row):
    age = row['Vict Age']
    if age is None:
        return None  
    if age < 18:
        age_group = "Children"
    elif 18 <= age <= 24:
        age_group = "Young Adults"
    elif 25 <= age <= 64:
        age_group = "Adults"
    else:
        age_group = "Elderly"
    return (age_group, 1)


crimes_old_rdd = crimes_old_df.rdd
crimes_new_rdd = crimes_new_df.rdd
crimes_rdd = crimes_old_rdd.union(crimes_new_rdd)

assaults_rdd = crimes_rdd.filter(
    lambda row: row['Crm Cd Desc'] and 'aggravated assault' in row['Crm Cd Desc'].lower()
)

counts_rdd = assaults_rdd \
    .map(map_to_age_groups) \
    .filter(lambda x: x is not None) \
    .reduceByKey(lambda a, b: a + b)

sorted_rdd = counts_rdd.map(lambda x: (x[1], x[0])) \
                       .sortByKey(ascending=False)

result_rdd = sorted_rdd.map(lambda x: (x[1], x[0]))

start_time = time.time()
result = result_rdd.collect()
end_time = time.time()
times.append(end_time - start_time)

print(f"RDD Time: {end_time - start_time:.4f} sec")
