


from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("wordcount example") \
    .getOrCreate() \
    .sparkContext

wordcount = sc.textFile("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/text.txt") \
    .flatMap(lambda x: x.split(" ")) \
    .map(lambda x: (x, 1)) \
    .reduceByKey(lambda x,y: x+y) \
    .sortBy(lambda x: x[1], ascending=False)

print(wordcount.collect())











# Implementation 1: RDD API
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext
    
# Load and process data
# CSV Columns: "id", "name", "salary", "dep_id"
employees = sc.textFile("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv") \
                .map(lambda x: (x.split(","))) # Split lines into a list of elements -> delimiter: ","
# print(employees.collect())

# Create tupples: (salary, [id, name, dep_id]); then sortByKey
# Αντιστοίχιση στηλών:
#   x[0] = id
#   x[1] = name
#   x[2] = salary
#   x[3] = dep_id
sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]]]) \
                    .sortByKey()

# Different output options
print(sorted_employees.take(5))
print("===========================================================================================================================")
for item in sorted_employees.coalesce(1).collect():
    print(item)






# Implementation 2: DataFrame API
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col

spark = SparkSession \
    .builder \
    .appName("DF query 1 execution") \
    .getOrCreate()

# Define the schema for the employees DataFrame
employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])


employees_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv", \
    header=False, \
    schema=employees_schema)
# Use "printSchema()" to display the datatypes of dataframes:
employees_df.printSchema()

# Alternative way to read csv:
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv")

sorted_employess_df = employees_df.sort(col("salary"))
# Use "explain()" to display physical plan:
sorted_employess_df.explain(mode="formatted")
sorted_employess_df.show(5)

print("===========================================================================================================================")
# To experiment more with the optimizer: 
print(sorted_employess_df._jdf.queryExecution().logical().toString()) #Get logical plan
print(sorted_employess_df._jdf.queryExecution().optimizedPlan().toString()) #Get optimized plan
print(sorted_employess_df._jdf.queryExecution().executedPlan().toString()) # Get physical plan








# Implementation 1: RDD API
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 2 execution") \
    .getOrCreate() \
    .sparkContext

    
employees = sc.textFile("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv") \
                .map(lambda x: (x.split(","))) # → [emp_id, emp_name, salary, dep_id]
departments = sc.textFile("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/departments.csv") \
                .map(lambda x: (x.split(","))) # → [id, dpt_name]

# =======================
# SCHEMA DETAILS:
# employees:   "emp_id", "emp_name", "salary", "dep_id"
# departments: "id", "dpt_name"
#
# Contents of employees RDD per row:
#   x[0] = emp_id
#   x[1] = emp_name
#   x[2] = salary
#   x[3] = dep_id
#
# Contents of departments RDD per row:
#   x[0] = id
#   x[1] = dpt_name
# =======================

# Filter & only keep departments named "Dep A"
depA = departments.filter(lambda x: x[1] == "Dep A")

# (k, v) -> (dep_id, [emp_id, emp_name, salary])
employees_formatted = employees.map(lambda x: [x[3] , [x[0],x[1],x[2]] ] )
# (k, v) -> (id, [dpt_name])
depA_formatted = depA.map(lambda x: [x[0], [x[1]]])
# print(employees_formatted.collect())
# print(depA_formatted.collect())


# Guess the data format????
joined_data = employees_formatted.join(depA_formatted)
# print(joined_data.collect())

get_employees = joined_data.map(lambda x: (x[1][0]))
# print(get_employees.collect())

sorted_employees= get_employees.map(lambda x: [int(x[2]),[x[0], x[1]] ] ) \
                                .sortByKey(ascending=False)
print(sorted_employees.take(3))


# Implementation 2: SQL API
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
spark = SparkSession \
    .builder \
    .appName("DF query 2 execution") \
    .getOrCreate()

employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees.csv")

departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
])

departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/departments.csv")

# To utilize as SQL tables
employees_df.createOrReplaceTempView("employees")
departments_df.createOrReplaceTempView("departments")

### USE TEMPORARY TABLE depA ###
id_query = "SELECT departments.id, departments.name \
    FROM departments \
    WHERE departments.name == 'Dep A'"

depA_id = spark.sql(id_query)
depA_id.createOrReplaceTempView("depA")
inner_join_query = "SELECT employees.name, employees.salary \
    FROM employees \
    INNER JOIN depA ON employees.dep_id == depA.id \
    ORDER BY employees.salary DESC"
joined_data = spark.sql(inner_join_query)
################################
### OR NOT ###
# inner_join_query = """
#     SELECT employees.name, employees.salary
#     FROM employees
#     INNER JOIN departments ON employees.dep_id == departments.id
#     WHERE departments.name == 'Dep A'
#     ORDER BY employees.salary DESC
# """
# joined_data = spark.sql(inner_join_query)
################################
joined_data.show(3)
joined_data.explain(mode="formatted")





# Implementation 1: RDD API
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 3 execution") \
    .getOrCreate() \
    .sparkContext
    
employees = sc.textFile("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees2.csv") \
                .map(lambda x: (x.split(",")))
# print(employees.collect())

employees_yearly = employees.map(lambda x: [x[1], 14*(int(x[2]))+int(x[4])])

                    
print(employees_yearly.collect())


# Implementation 2: DataFrame API
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col, udf


spark = SparkSession.builder \
    .appName("DataFrame query 3 execution (UDF example)") \
    .getOrCreate()
()
employees2_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
    StructField("bonus", FloatType())
])


employees_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/example_data/employees2.csv", header=False, schema=employees2_schema)



###  WITH UDF  ###
def calculate_yearly_income(salary, bonus):
    return 14*salary+bonus
# Register the UDF
calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())
employees_yearly_df = employees_df \
    .withColumn("yearly", calculate_yearly_income_udf(col("salary"), col("bonus"))).select("name", "yearly")
##################

### WITHOUT UDF ###
# employees_yearly_df = employees_df \
#     .withColumn("yearly", (14*col("salary")+col("bonus"))).select("name", "yearly")
####################

employees_yearly_df.show()



