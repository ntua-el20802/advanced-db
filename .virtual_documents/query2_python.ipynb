get_ipython().run_cell_magic("configure", " -f", """{
    "driverMemory": "2G",
    "executorMemory": "2G",
    "executorCores": 1,
    "numExecutors": 4
}""")


from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("query2_python").getOrCreate()
sc = spark.sparkContext



crimes_old_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv",\
    header=True,
    inferSchema=True)
crimes_new_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv",\
    header=True,
    inferSchema=True)


descent_diction = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv",\
    header=True,
    inferSchema=True)


from pyspark.sql import functions as F
from pyspark.sql import Window
import time

runs = 10


# descent_diction.show(5,False)

year_df = crimes_df.withColumn(
    "year",
    F.year(F.to_timestamp("DATE OCC", "yyyy MMM dd hh:mm:ss a"))
)

year_df.select("year").show(2)

clean_df = year_df.filter(
    (F.col("year").isNotNull()) &
    (F.col("Vict Descent").isNotNull())
)

per_race = (
    clean_df
      .groupBy("year", "Vict Descent")
      .agg(F.count("*").alias("cnt"))
)

# per_race.show(5)

# Window: total victims per year

w_year = Window.partitionBy("year")

ranked = per_race.withColumn(
    "year_total",
    F.sum("cnt").over(w_year)
)

#  Percentage per year

ranked = ranked.withColumn(
    "percent",
    F.round(F.col("cnt") / F.col("year_total") * 100, 1)
)

#  Window: rank per year (top-3 per race)

w_rank = Window.partitionBy("year").orderBy(F.desc("cnt"))

ranked = ranked.withColumn(
    "rank",
    F.dense_rank().over(w_rank)
)

#  Keep only Top-3 per year

top3 = ranked.filter(F.col("rank") <= 3)


# top3.show(5,False)

#lets join with Racial codes

result = top3.join(
    descent_diction,
    on="Vict Descent",
    how="left"
)

# result.show(5,False)


#Final view

result = (
    result
      .select(
          "year",
          F.col("Vict Descent Full").alias("Vict_Descent"),
          "cnt",
          "percent"
      )
      .orderBy(F.desc("year"), F.desc("cnt"))
)

result.collect()



warmups = 3
runs = 10
times = []

for i in range(warmups + runs):

    crimes_df = crimes_old_df.unionByName(crimes_new_df)

    year_df = crimes_df.withColumn(
        "year",
        F.year(F.to_timestamp(F.col("DATE OCC"), "yyyy MMM dd hh:mm:ss a"))
    )

    clean_df = year_df.filter(
        F.col("year").isNotNull() & F.col("Vict Descent").isNotNull()
    )

    per_race = (
        clean_df.groupBy("year", "Vict Descent")
        .agg(F.count("*").alias("cnt"))
    )

    w_year = Window.partitionBy("year")
    ranked = per_race.withColumn("year_total", F.sum("cnt").over(w_year))

    ranked = ranked.withColumn(
        "percent",
        F.round(F.col("cnt") / F.col("year_total") * 100, 1)
    )

    w_rank = Window.partitionBy("year").orderBy(F.desc("cnt"))
    ranked = ranked.withColumn("rank", F.dense_rank().over(w_rank))

    top3 = ranked.filter(F.col("rank") <= 3)

    # πιο σταθερό join (αν το RE_codes είναι μικρό)
    result = top3.join(descent_diction, on="Vict Descent", how="left")

    result = (
        result.select(
            "year",
            F.col("Vict Descent Full").alias("Vict_Descent"),
            "cnt",
            "percent"
        )
        .orderBy(F.desc("year"), F.desc("cnt"))
    )

    start = time.time()
    rows = result.collect()
    dt = time.time() - start

    del rows
    gc.collect()

    if i < warmups:
        print(f"Warm-up {i+1}: {dt:.4f} sec")
    else:
        times.append(dt)
        print(f"Run {i-warmups+1}: {dt:.4f} sec")

mean_time = sum(times) / len(times)
print(f"Mean time (excluding warm-ups): {mean_time:.4f} sec")
