get_ipython().run_cell_magic("configure", " -f", """{
    "driverMemory": "2G",
    "executorMemory": "8G",
    "executorCores": 4,
    "numExecutors": 2
}""")


from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("query4_1").getOrCreate()
sc = spark.sparkContext


crimes_old_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv",\
    header=True,
    inferSchema=True)
crimes_new_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv",\
    header=True,
    inferSchema=True)

crimes_df = crimes_old_df.unionByName(crimes_new_df)

stations_df = spark.read.csv("s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv",\
    header=True,
    inferSchema=True)



from sedona.spark import *
from pyspark.sql import functions as F
from pyspark.sql.functions import col
import time, gc

# Create/Init Sedona
spark = SedonaContext.create(spark)

# ----------------------------
# Query 4 (DataFrame API)
# ----------------------------

# Rename X,Y -> lon,lat
stations_geo = (
    stations_df
    .withColumnRenamed("X", "lon")
    .withColumnRenamed("Y", "lat")
)

# Filter crimes: remove nulls + "Null Island"
crimes_df_clean = crimes_df.filter(
    F.col("LAT").isNotNull() &
    F.col("LON").isNotNull() &
    ~((F.col("LAT") == 0) & (F.col("LON") == 0))
)

# Keep only needed columns
crimes_geo = crimes_df_clean.select("AREA", "LAT", "LON")

# Create geometries
stations_geo = stations_geo.withColumn("station_geom", ST_Point(col("lon"), col("lat")))
crimes_geo   = crimes_geo.withColumn("crime_geom",   ST_Point(col("LON"), col("LAT")))

# Join on AREA = PREC
joined_df = crimes_geo.alias("c").join(
    stations_geo.alias("s"),
    col("c.AREA") == col("s.PREC"),
    "inner"
)

final_df = joined_df.select(
    col("s.FID").alias("FID"),
    col("s.DIVISION").alias("DIVISION"),
    col("s.LOCATION").alias("LOCATION"),
    col("s.PREC").alias("PREC"),
    col("s.station_geom").alias("station_geom"),
    col("c.crime_geom").alias("crime_geom")
)

with_dist = final_df.withColumn(
    "distance_m",
    ST_DistanceSphere(col("station_geom"), col("crime_geom"))
)

result = (
    with_dist
    .groupBy("DIVISION")
    .agg(
        F.round(F.avg("distance_m"), 3).alias("average_distance"),
        F.count("*").alias("count")
    )
    .orderBy(F.desc("count"))
)

# ----------------------------
# Timing: 3 warm-ups + 5 runs
# ----------------------------
warmups = 3
runs = 5
times = []

for i in range(warmups + runs):
    start = time.time()
    rows = result.collect()   # action
    dt = time.time() - start

    del rows
    gc.collect()

    if i < warmups:
        print(f"Warm-up {i+1}: {dt:.4f} sec")
    else:
        times.append(dt)
        print(f"Run {i-warmups+1}: {dt:.4f} sec")

mean_time = sum(times) / len(times)
print(f"Mean time (excluding warm-ups): {mean_time:.4f} sec")

# ----------------------------
# Explain only at the end
# ----------------------------

joined_df.explain("formatted")

# Optional: show sample output
result.show(5, truncate=False)








