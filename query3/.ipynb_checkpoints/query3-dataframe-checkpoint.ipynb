{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4dc845-20dc-44a5-9268-c43f7b87e6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive'}, 'driverMemory': '2G', 'executorMemory': '2G', 'executorCores': 1, 'numExecutors': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1136</td><td>application_1765289937462_1129</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1129/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1129_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1192</td><td>application_1765289937462_1185</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1185/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1185_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1196</td><td>application_1765289937462_1189</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1189/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1189_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1197</td><td>application_1765289937462_1190</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1190/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1190_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1198</td><td>application_1765289937462_1191</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1191/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1191_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"2G\",\n",
    "    \"executorMemory\": \"2G\",\n",
    "    \"executorCores\": 1,\n",
    "    \"numExecutors\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f6189c-26d3-4155-b59a-3a5223b8e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1199</td><td>application_1765289937462_1192</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1192/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1192_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4329eaf5dfc44cd91463221ec05fe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676ca6fa461f411c83f9b5cd86022725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: 4\n",
      "Master: yarn"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "from pyspark.sql.functions import col, desc, explode, split\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Executors: {sc.getConf().get('spark.executor.instances')}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9ea7a0-1ab7-48a3-bd24-efedb349aa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3ac3c68a734591a41c501139d4f305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615"
     ]
    }
   ],
   "source": [
    "path_crimes_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "path_crimes_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "path_mocodes = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    "\n",
    "df_crimes = spark.read.option(\"header\", \"true\")\\\n",
    "    .csv([path_crimes_1, path_crimes_2])\n",
    "\n",
    "mo_codes = spark.read.text(path_mocodes) \\\n",
    "                .select(split(col(\"value\"), \" \", 2).alias(\"parts\")) \\\n",
    "                .select(\n",
    "                    col(\"parts\")[0].alias(\"code\"),\n",
    "                    col(\"parts\")[1].alias(\"description\")\n",
    "                )\n",
    "\n",
    "df_crimes.rdd.count()\n",
    "mo_codes.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4673ca-8859-4a09-ae25-5e677221ddf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b78e28250b44d99e242ca7eb1c5350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|code| count|\n",
      "+----+------+\n",
      "|0543|   221|\n",
      "|0401| 13049|\n",
      "|1280|    29|\n",
      "|0201|   608|\n",
      "|1008|  1071|\n",
      "|0371| 15199|\n",
      "|0385| 41926|\n",
      "|0908|  1837|\n",
      "|0535|    82|\n",
      "|0514|  1034|\n",
      "|1805|   910|\n",
      "|1314|   449|\n",
      "|1300|219082|\n",
      "|0394| 25369|\n",
      "|0409|  1408|\n",
      "|1501|115589|\n",
      "|1013|   302|\n",
      "|1405|   280|\n",
      "|1303|    81|\n",
      "|0379|   294|\n",
      "+----+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "crime_counts = df_crimes.filter(col(\"Mocodes\").isNotNull()) \\\n",
    "                        .select(explode(split(col(\"Mocodes\"), \" \")).alias(\"code\")) \\\n",
    "                        .filter(col(\"code\") != \"\") \\\n",
    "                        .groupBy(\"code\") \\\n",
    "                        .count()\n",
    "\n",
    "crime_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb87f7c3-128a-48e0-afc2-601139db9e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f092d2bbfdb24777a105b5c424f07d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# times = []\n",
    "# strategies = [\"BROADCAST\", \"MERGE\", \"SHUFFLE_HASH\", \"SHUFFLE_REPLICATE_NL\"]\n",
    "\n",
    "# for strategy in strategies:\n",
    "#     joined_df = crime_counts.join(mo_codes.hint(strategy), \"code\") \\\n",
    "#         .orderBy(desc(\"count\"))\n",
    "\n",
    "#     print(f\"\\n--- Plan for {strategy} ---\")\n",
    "#     joined_df.explain()\n",
    "\n",
    "#     start = time.time()\n",
    "#     joined_df.rdd.count()\n",
    "#     end = time.time()\n",
    "\n",
    "#     times.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7552e6f9-cff5-4d5c-b2d3-6e635f4d57d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0958d05f054e70b732c3a4d12c8b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plan for BROADCAST ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#129L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#129L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=6898]\n",
      "      +- Project [code#125, count#129L, description#91]\n",
      "         +- BroadcastHashJoin [code#125], [code#90], Inner, BuildRight, false\n",
      "            :- HashAggregate(keys=[code#125], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(code#125, 1000), ENSURE_REQUIREMENTS, [plan_id=6891]\n",
      "            :     +- HashAggregate(keys=[code#125], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter NOT (code#125 = )\n",
      "            :           +- Generate explode(split(Mocodes#39,  , -1)), false, [code#125]\n",
      "            :              +- Filter isnotnull(Mocodes#39)\n",
      "            :                 +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6894]\n",
      "               +- Project [parts#87[0] AS code#90, parts#87[1] AS description#91]\n",
      "                  +- Project [split(value#85,  , 2) AS parts#87]\n",
      "                     +- Filter (NOT (split(value#85,  , 2)[0] = ) AND isnotnull(split(value#85,  , 2)[0]))\n",
      "                        +- FileScan text [value#85] Batched: false, DataFilters: [NOT (split(value#85,  , 2)[0] = ), isnotnull(split(value#85,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "DataFrame[code: string, count: bigint, description: string]\n",
      "\n",
      "--- Plan for MERGE ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#129L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#129L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=7120]\n",
      "      +- Project [code#125, count#129L, description#91]\n",
      "         +- SortMergeJoin [code#125], [code#90], Inner\n",
      "            :- Sort [code#125 ASC NULLS FIRST], false, 0\n",
      "            :  +- HashAggregate(keys=[code#125], functions=[count(1)], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(code#125, 1000), ENSURE_REQUIREMENTS, [plan_id=7110]\n",
      "            :        +- HashAggregate(keys=[code#125], functions=[partial_count(1)], schema specialized)\n",
      "            :           +- Filter NOT (code#125 = )\n",
      "            :              +- Generate explode(split(Mocodes#39,  , -1)), false, [code#125]\n",
      "            :                 +- Filter isnotnull(Mocodes#39)\n",
      "            :                    +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "            +- Sort [code#90 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(code#90, 1000), ENSURE_REQUIREMENTS, [plan_id=7114]\n",
      "                  +- Project [parts#87[0] AS code#90, parts#87[1] AS description#91]\n",
      "                     +- Project [split(value#85,  , 2) AS parts#87]\n",
      "                        +- Filter (NOT (split(value#85,  , 2)[0] = ) AND isnotnull(split(value#85,  , 2)[0]))\n",
      "                           +- FileScan text [value#85] Batched: false, DataFilters: [NOT (split(value#85,  , 2)[0] = ), isnotnull(split(value#85,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "DataFrame[code: string, count: bigint, description: string]\n",
      "\n",
      "--- Plan for SHUFFLE_HASH ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#129L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#129L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=7391]\n",
      "      +- Project [code#125, count#129L, description#91]\n",
      "         +- ShuffledHashJoin [code#125], [code#90], Inner, BuildRight\n",
      "            :- HashAggregate(keys=[code#125], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(code#125, 1000), ENSURE_REQUIREMENTS, [plan_id=7383]\n",
      "            :     +- HashAggregate(keys=[code#125], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter NOT (code#125 = )\n",
      "            :           +- Generate explode(split(Mocodes#39,  , -1)), false, [code#125]\n",
      "            :              +- Filter isnotnull(Mocodes#39)\n",
      "            :                 +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "            +- Exchange hashpartitioning(code#90, 1000), ENSURE_REQUIREMENTS, [plan_id=7387]\n",
      "               +- Project [parts#87[0] AS code#90, parts#87[1] AS description#91]\n",
      "                  +- Project [split(value#85,  , 2) AS parts#87]\n",
      "                     +- Filter (NOT (split(value#85,  , 2)[0] = ) AND isnotnull(split(value#85,  , 2)[0]))\n",
      "                        +- FileScan text [value#85] Batched: false, DataFilters: [NOT (split(value#85,  , 2)[0] = ), isnotnull(split(value#85,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "DataFrame[code: string, count: bigint, description: string]\n",
      "\n",
      "--- Plan for SHUFFLE_REPLICATE_NL ---\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#129L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#129L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=7619]\n",
      "      +- Project [code#125, count#129L, description#91]\n",
      "         +- CartesianProduct (code#125 = code#90)\n",
      "            :- HashAggregate(keys=[code#125], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(code#125, 1000), ENSURE_REQUIREMENTS, [plan_id=7614]\n",
      "            :     +- HashAggregate(keys=[code#125], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter NOT (code#125 = )\n",
      "            :           +- Generate explode(split(Mocodes#39,  , -1)), false, [code#125]\n",
      "            :              +- Filter isnotnull(Mocodes#39)\n",
      "            :                 +- FileScan csv [Mocodes#39] Batched: false, DataFilters: [isnotnull(Mocodes#39)], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(Mocodes)], ReadSchema: struct<Mocodes:string>\n",
      "            +- Project [parts#87[0] AS code#90, parts#87[1] AS description#91]\n",
      "               +- Project [split(value#85,  , 2) AS parts#87]\n",
      "                  +- Filter (NOT (split(value#85,  , 2)[0] = ) AND isnotnull(split(value#85,  , 2)[0]))\n",
      "                     +- FileScan text [value#85] Batched: false, DataFilters: [NOT (split(value#85,  , 2)[0] = ), isnotnull(split(value#85,  , 2)[0])], Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "\n",
      "\n",
      "DataFrame[code: string, count: bigint, description: string]"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "strategies = [\n",
    "    \"BROADCAST\",\n",
    "    \"MERGE\",\n",
    "    \"SHUFFLE_HASH\",\n",
    "    \"SHUFFLE_REPLICATE_NL\"\n",
    "]\n",
    "\n",
    "for strategy in strategies:\n",
    "\n",
    "    # Create fresh DataFrames for this strategy\n",
    "    left_df = crime_counts\n",
    "    right_df = mo_codes.hint(strategy)\n",
    "\n",
    "    joined_df = (\n",
    "        left_df\n",
    "        .join(right_df, \"code\")\n",
    "        .orderBy(desc(\"count\"))\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Plan for {strategy} ---\")\n",
    "    joined_df.explain()\n",
    "\n",
    "    start_time = time.time()\n",
    "    joined_df.foreach(lambda _: None)\n",
    "    end_time = time.time()\n",
    "\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "    joined_df.unpersist(blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c378dd-064b-4cf8-9965-ca0b6175428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ee62d959424d44a8d9a98c68525ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy BROADCAST : 16.31821346282959 sec\n",
      "Strategy MERGE : 9.290767669677734 sec\n",
      "Strategy SHUFFLE_HASH : 7.342074632644653 sec\n",
      "Strategy SHUFFLE_REPLICATE_NL : 5.557344436645508 sec"
     ]
    }
   ],
   "source": [
    "for i, strategy in enumerate(strategies):\n",
    "    print(f\"Strategy {strategy} : {times[i]} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a257e8-a975-4e23-9a1b-58296e8a2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb8ba9c73e24c2ca88596fa182dd7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'times' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'times' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mean = sum(times) / len(strategies)\n",
    "print(f\"mean time for dataframes {dataframe_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
